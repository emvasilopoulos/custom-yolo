{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import pathlib\n",
    "dataset_path = pathlib.Path(\"/home/emvasilopoulos/projects/datasets/coco128\")\n",
    "images_path = dataset_path / \"images\"\n",
    "train_images_path = images_path / \"train2017\"\n",
    "val_images_path = images_path / \"val2017\"\n",
    "annotations_path = dataset_path / \"annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Download (Run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.dataset.coco.downloader\n",
    "\n",
    "images_path.mkdir(parents=True, exist_ok=True)\n",
    "annotations_path.mkdir(parents=True, exist_ok=True)\n",
    "if not list(annotations_path.glob(\"*.json\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_train_val_annotations_2017(annotations_path)\n",
    "if not list(train_images_path.glob(\"*.jpg\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_train_images_2017(train_images_path, val_images_path)\n",
    "if not list(val_images_path.glob(\"*.jpg\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_val_images_2017(val_images_path, train_images_path)\n",
    "\n",
    "print(\"Manually unzip and organize the downloaded data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Parse Raw COCO to repo's format (Run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.dataset.coco.raw_annotations_parser\n",
    "import custom_yolo_lib.dataset.coco.tasks.utils\n",
    "\n",
    "val_annotations_path = annotations_path / \"instances_val2017.json\"\n",
    "train_annotations_path = annotations_path / \"instances_train2017.json\"\n",
    "for split in [\"val\", \"train\"]:\n",
    "    p = annotations_path / f\"instances_{split}2017.json\"\n",
    "    raw_parser = custom_yolo_lib.dataset.coco.raw_annotations_parser.RawCOCOAnnotationsParser(p)\n",
    "    raw_parser.parse_data()\n",
    "    filename = custom_yolo_lib.dataset.coco.tasks.utils.get_task_file(\n",
    "        \"instances\",\n",
    "        split,\n",
    "        \"2017\",\n",
    "        is_grouped=True,\n",
    "        filetype=custom_yolo_lib.dataset.coco.tasks.utils.AnnotationsType.json\n",
    "    )\n",
    "    grouped_annotations_path = p.parent / filename\n",
    "    raw_parser.write_data(grouped_annotations_path)\n",
    "    custom_yolo_lib.dataset.coco.tasks.utils.convert_grouped_instances_json_to_csv(grouped_annotations_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Model, Dataset, Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import custom_yolo_lib.model.bundled\n",
    "import custom_yolo_lib.dataset.coco.tasks.instances\n",
    "import custom_yolo_lib.image_size\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "image_size = custom_yolo_lib.image_size.ImageSize(640, 640)\n",
    "\n",
    "model = custom_yolo_lib.model.bundled.YOLOModel(80, training=True)\n",
    "model.to(device)\n",
    "\n",
    "val_dataset = custom_yolo_lib.dataset.coco.tasks.instances.COCOInstances2017(dataset_path, \"val\", expected_image_size=image_size, device=device)\n",
    "train_dataset = custom_yolo_lib.dataset.coco.tasks.instances.COCOInstances2017(dataset_path, \"train\", expected_image_size=image_size, device=device)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 85, 80, 80])\n",
      "torch.Size([2, 85, 40, 40])\n",
      "torch.Size([2, 85, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "val_dataset = custom_yolo_lib.dataset.coco.tasks.instances.COCOInstances2017(dataset_path, \"val\", expected_image_size=image_size, device=device)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "for i, (images, targets) in enumerate(validation_loader):\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    predictions_s, predictions_m, predictions_l = model(images)\n",
    "    print(predictions_s.anchor1_output.shape)\n",
    "    print(predictions_m.anchor2_output.shape)\n",
    "    print(predictions_l.anchor3_output.shape)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
