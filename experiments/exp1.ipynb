{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: COCO is very bad for training. It has countless mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import pathlib\n",
    "dataset_path = pathlib.Path(\"/home/manos/custom-yolo/coco_data\")\n",
    "images_path = dataset_path / \"images\"\n",
    "train_images_path = images_path / \"train2017\"\n",
    "val_images_path = images_path / \"val2017\"\n",
    "annotations_path = dataset_path / \"annotations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Download (Run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.dataset.coco.downloader\n",
    "\n",
    "images_path.mkdir(parents=True, exist_ok=True)\n",
    "annotations_path.mkdir(parents=True, exist_ok=True)\n",
    "if not list(annotations_path.glob(\"*.json\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_train_val_annotations_2017(annotations_path)\n",
    "if not list(train_images_path.glob(\"*.jpg\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_train_images_2017(train_images_path, val_images_path)\n",
    "if not list(val_images_path.glob(\"*.jpg\")):\n",
    "    custom_yolo_lib.dataset.coco.downloader.download_val_images_2017(val_images_path, train_images_path)\n",
    "\n",
    "print(\"Manually unzip and organize the downloaded data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Parse Raw COCO to repo's format (Run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.dataset.coco.raw_annotations_parser\n",
    "import custom_yolo_lib.dataset.coco.tasks.utils\n",
    "\n",
    "val_annotations_path = annotations_path / \"instances_val2017.json\"\n",
    "train_annotations_path = annotations_path / \"instances_train2017.json\"\n",
    "for split in [\"val\", \"train\"]:\n",
    "    p = annotations_path / f\"instances_{split}2017.json\"\n",
    "    raw_parser = custom_yolo_lib.dataset.coco.raw_annotations_parser.RawCOCOAnnotationsParser(p)\n",
    "    raw_parser.parse_data()\n",
    "    filename = custom_yolo_lib.dataset.coco.tasks.utils.get_task_file(\n",
    "        \"instances\",\n",
    "        split,\n",
    "        \"2017\",\n",
    "        is_grouped=True,\n",
    "        filetype=custom_yolo_lib.dataset.coco.tasks.utils.AnnotationsType.json\n",
    "    )\n",
    "    grouped_annotations_path = p.parent / filename\n",
    "    raw_parser.write_data(grouped_annotations_path)\n",
    "    custom_yolo_lib.dataset.coco.tasks.utils.convert_grouped_instances_json_to_csv(grouped_annotations_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Model, Dataset, Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.training.utils\n",
    "import custom_yolo_lib.training.lr_scheduler\n",
    "import custom_yolo_lib.image_size\n",
    "import custom_yolo_lib.model.e2e.anchor_based.bundled_anchor_based\n",
    "\n",
    "import torch\n",
    "\n",
    "epochs = 300\n",
    "num_classes = 80\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "image_size = custom_yolo_lib.image_size.ImageSize(640, 640)\n",
    "\n",
    "model = custom_yolo_lib.model.e2e.anchor_based.bundled_anchor_based.YOLOModel(num_classes=num_classes, training=True)\n",
    "model.to(device)\n",
    "parameters_grouped = custom_yolo_lib.training.utils.get_params_grouped(model)\n",
    "lr = 0.001\n",
    "momentum = 0.937\n",
    "decay = 0.001\n",
    "batch_size = 8\n",
    "optimizer = torch.optim.AdamW(\n",
    "    parameters_grouped.bias, lr=lr, betas=(momentum, 0.999), weight_decay=0.0\n",
    ")\n",
    "optimizer.add_param_group({\"params\": parameters_grouped.weight_decay, \"weight_decay\": decay})\n",
    "optimizer.add_param_group({\"params\": parameters_grouped.no_weight_decay, \"weight_decay\": 0.0})\n",
    "scheduler = custom_yolo_lib.training.lr_scheduler.StepLRScheduler(\n",
    "    optimizer, update_step_size=10000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_yolo_lib.dataset.coco.tasks.instances\n",
    "import custom_yolo_lib.dataset.coco.tasks.loader\n",
    "\n",
    "classes = [i for i in range(num_classes)]\n",
    "\n",
    "val_dataset = custom_yolo_lib.dataset.coco.tasks.instances.COCOInstances2017(dataset_path, \"val\", expected_image_size=image_size, classes=classes)\n",
    "train_dataset = custom_yolo_lib.dataset.coco.tasks.instances.COCOInstances2017(dataset_path, \"train\", expected_image_size=image_size, classes=classes)\n",
    "training_loader = custom_yolo_lib.dataset.coco.tasks.loader.COCODataLoaderThreeFeatureMaps(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_loader = custom_yolo_lib.dataset.coco.tasks.loader.COCODataLoaderThreeFeatureMaps(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_yolo_lib.dataset.coco.constants import MEDIUM_AREA_RANGE, SMALL_AREA_RANGE\n",
    "import custom_yolo_lib.model.e2e.anchor_based.loss\n",
    "import custom_yolo_lib.model.e2e.anchor_based.training_utils\n",
    "import custom_yolo_lib.dataset.coco.tasks.loader\n",
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "SMALL_AREA_UPPER_BOUND = SMALL_AREA_RANGE[1]\n",
    "MEDIUM_AREA_LOWER_BOUND = MEDIUM_AREA_RANGE[0]\n",
    "MEDIUM_AREA_UPPER_BOUND = MEDIUM_AREA_RANGE[1]\n",
    "LARGE_AREA_LOWER_BOUND = MEDIUM_AREA_UPPER_BOUND\n",
    "\n",
    "small_map_anchors, medium_map_anchors, large_map_anchors = custom_yolo_lib.model.e2e.anchor_based.training_utils.get_anchors_as_bbox_tensors(device)\n",
    "loss_s = custom_yolo_lib.model.e2e.anchor_based.loss.YOLOLossPerFeatureMap(\n",
    "    num_classes=num_classes,\n",
    "    feature_map_anchors=small_map_anchors,\n",
    ")\n",
    "loss_m = custom_yolo_lib.model.e2e.anchor_based.loss.YOLOLossPerFeatureMap(\n",
    "    num_classes=num_classes,\n",
    "    feature_map_anchors=medium_map_anchors,\n",
    ")\n",
    "loss_l = custom_yolo_lib.model.e2e.anchor_based.loss.YOLOLossPerFeatureMap(\n",
    "    num_classes=num_classes,\n",
    "    feature_map_anchors=large_map_anchors,\n",
    ")\n",
    "\n",
    "coco_batch: custom_yolo_lib.dataset.coco.tasks.loader.COCODataLoaderThreeFeatureMapBatch\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # TRAINING \n",
    "    training_session_data = {\n",
    "        \"bbox_loss_avg_featmap\": [],\n",
    "        \"objectness_loss_avg_featmap\": [],\n",
    "        \"class_loss_avg_featmap\": [],\n",
    "        \"total_loss_avg_featmap\": [],\n",
    "        \"epoch\": [],\n",
    "        \"step\": [],\n",
    "    }\n",
    "    tqdm_obj = tqdm.tqdm(training_loader)\n",
    "    model.train()\n",
    "    for i, coco_batch in enumerate(tqdm_obj):\n",
    "\n",
    "        images = coco_batch.images_batch.to(device)\n",
    "        targets_s = [t.to(device) for t in coco_batch.small_objects_batch]\n",
    "        targets_m = [t.to(device) for t in coco_batch.medium_objects_batch]\n",
    "        targets_l = [t.to(device) for t in coco_batch.large_objects_batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions_s, predictions_m, predictions_l = model(images)\n",
    "\n",
    "        loss_s_ = loss_s(predictions_s, targets_s)\n",
    "        loss_m_ = loss_m(predictions_m, targets_m)\n",
    "        loss_l_ = loss_l(predictions_l, targets_l)\n",
    "        loss = (loss_s_[3] + loss_m_[3] + loss_l_[3]) / 3\n",
    "        loss.backward()\n",
    "\n",
    "        scheduler.update_loss(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_bbox_loss = (loss_s_[0] + loss_m_[0] + loss_l_[0]) / 3\n",
    "        avg_objectness_loss = (loss_s_[1] + loss_m_[1] + loss_l_[1]) / 3\n",
    "        avg_class_loss = (loss_s_[2] + loss_m_[2] + loss_l_[2]) / 3\n",
    "        tqdm_obj.set_description(f\"Training | Total Loss: {loss.item():.4f} | BBox Loss: {avg_bbox_loss.item():.4f} | Obj Loss: {avg_objectness_loss.item():.4f} | Class Loss: {avg_class_loss.item():.4f}\")\n",
    "        training_session_data[\"bbox_loss_avg_featmap\"].append(avg_bbox_loss.item())\n",
    "        training_session_data[\"objectness_loss_avg_featmap\"].append(avg_objectness_loss.item())\n",
    "        training_session_data[\"class_loss_avg_featmap\"].append(avg_class_loss.item())\n",
    "        training_session_data[\"total_loss_avg_featmap\"].append(loss.item())\n",
    "        training_session_data[\"epoch\"].append(epoch)\n",
    "        training_session_data[\"step\"].append(i)\n",
    "        for i, lr in enumerate(scheduler.get_lr()):\n",
    "            if f\"lr-{i}\" not in training_session_data:\n",
    "                training_session_data[f\"lr-{i}\"] = []\n",
    "            training_session_data[f\"lr-{i}\"].append(lr)\n",
    "\n",
    "    # Store training data\n",
    "    model_state = model.state_dict()\n",
    "    torch.save(model_state, f\"model_epoch_{epoch}.pth\")\n",
    "    pd.DataFrame(training_session_data).to_csv(f\"training_session_data_epoch_{epoch}.csv\")\n",
    "\n",
    "\n",
    "    # VALIDATION\n",
    "    validation_session_data = {\n",
    "        \"bbox_loss_avg_featmap\": [],\n",
    "        \"objectness_loss_avg_featmap\": [],\n",
    "        \"class_loss_avg_featmap\": [],\n",
    "        \"total_loss_avg_featmap\": [],\n",
    "        \"epoch\": [],\n",
    "        \"step\": []\n",
    "    }\n",
    "    tqdm_obj = tqdm.tqdm(validation_loader)\n",
    "    model.eval()\n",
    "    model.training = True\n",
    "    with torch.no_grad():\n",
    "        for i, coco_batch in enumerate(tqdm_obj):\n",
    "            \"\"\"\n",
    "            targets.shape = (batch_size, MAX_ALLOWED_OBJECTS, 5 + num_classes) | 5-->(x, y, w, h, objectness)\n",
    "            targets_n_objects.shape = (batch_size, 1)\n",
    "            \"\"\"\n",
    "\n",
    "            images = coco_batch.images_batch.to(device)\n",
    "            targets_s = [t.to(device) for t in coco_batch.small_objects_batch]\n",
    "            targets_m = [t.to(device) for t in coco_batch.medium_objects_batch]\n",
    "            targets_l = [t.to(device) for t in coco_batch.large_objects_batch]\n",
    "            predictions_s, predictions_m, predictions_l = model(images) # training=True is needed to get the predictions in the same format as the targets\n",
    "\n",
    "            loss_s_ = loss_s(predictions_s, targets_s)\n",
    "            loss_m_ = loss_m(predictions_m, targets_m)\n",
    "            loss_l_ = loss_l(predictions_l, targets_l)\n",
    "\n",
    "            loss = (loss_s_[3] + loss_m_[3] + loss_l_[3]) / 3\n",
    "            avg_bbox_loss = (loss_s_[0] + loss_m_[0] + loss_l_[0]) / 3\n",
    "            avg_objectness_loss = (loss_s_[1] + loss_m_[1] + loss_l_[1]) / 3\n",
    "            avg_class_loss = (loss_s_[2] + loss_m_[2] + loss_l_[2]) / 3\n",
    "            tqdm_obj.set_description(f\"Validation | Total Loss: {loss.item():.4f} | BBox Loss: {avg_bbox_loss.item():.4f} | Obj Loss: {avg_objectness_loss.item():.4f} | Class Loss: {avg_class_loss.item():.4f}\")\n",
    "            validation_session_data[\"bbox_loss_avg_featmap\"].append(avg_bbox_loss.item())\n",
    "            validation_session_data[\"objectness_loss_avg_featmap\"].append(avg_objectness_loss.item())\n",
    "            validation_session_data[\"class_loss_avg_featmap\"].append(avg_class_loss.item())\n",
    "            validation_session_data[\"total_loss_avg_featmap\"].append(loss.item())\n",
    "            validation_session_data[\"epoch\"].append(epoch)\n",
    "            validation_session_data[\"step\"].append(i)\n",
    "    # Store validation data\n",
    "    pd.DataFrame(validation_session_data).to_csv(f\"validation_session_data_epoch_{epoch}.csv\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
